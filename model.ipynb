{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow.contrib.slim as slim\n",
    "import numpy as np\n",
    "from tensorflow.python.ops import control_flow_ops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 100\n",
    "BATCH_NUM = 100\n",
    "FEATURE_SIZE = 100\n",
    "EMBEDDED_SIZE = 3\n",
    "NUM_CHANNELS = 1\n",
    "CLASS_SIZE = 100\n",
    "LEARNING_RATE_BASE = 0.1\n",
    "LEARNING_RATE_DECAY = 0.99\n",
    "TRAIN_SIZE = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def random_shuffle(data, label):\n",
    "    idx = np.random.permutation(len(data))\n",
    "    random_data, random_label = data[idx], label[idx]\n",
    "    return random_data, random_label\n",
    "\n",
    "def preprocess_label(label, size):\n",
    "    label_one_hot = np.zeros(size)\n",
    "    label_one_hot[label] = 1\n",
    "    return label_one_hot\n",
    "    \n",
    "def preprocess_labels(labels, size):\n",
    "    labels_one_hot = []\n",
    "    for label in labels:\n",
    "        label_one_hot = np.zeros(size)\n",
    "        label_one_hot[label] = 1\n",
    "        labels_one_hot.append(label_one_hot)\n",
    "    return labels_one_hot\n",
    "    \n",
    "def generate_input_list(ent_embedding, rel_embedding, data_id):\n",
    "    hrs = []\n",
    "    rts = []\n",
    "    label_hrs = []\n",
    "    label_rts = []\n",
    "    zero_array = np.zeros(100)\n",
    "    for triplet_id in data_id:\n",
    "        h = ent_embedding[triplet_id[0]]\n",
    "        r = rel_embedding[triplet_id[2]]\n",
    "        t = ent_embedding[triplet_id[1]]\n",
    "        hr = np.transpose([[h, r, zero_array]])\n",
    "        rt = np.transpose([[zero_array, r, t]])\n",
    "        hrs.append(hr)\n",
    "        rts.append(rt)\n",
    "        label_hrs.append(triplet_id[1])\n",
    "        label_rts.append(triplet_id[1])\n",
    "    input_list = np.concatenate((hrs, rts), axis=0)\n",
    "    label_list = np.concatenate((label_hrs, label_rts), axis=0)\n",
    "    return input_list, label_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def model():\n",
    "    '''Create the Model\n",
    "    '''\n",
    "    \n",
    "    #input\n",
    "    # []: dimension\n",
    "    \n",
    "    x = tf.placeholder(tf.float32, [BATCH_SIZE,\n",
    "                                   FEATURE_SIZE,\n",
    "                                   EMBEDDED_SIZE,\n",
    "                                   NUM_CHANNELS],\n",
    "                          name = 'x-input')\n",
    "    # x_hat = tf.placeholder(tf.float32, [BATCH_SIZE,\n",
    "    #                                FEATURE_SIZE,\n",
    "    #                                EMBEDDED_SIZE,\n",
    "    #                                NUM_CHANNELS],\n",
    "    #                       name = 'x_hat-input')\n",
    "\n",
    "    y_gt = tf.placeholder(tf.float32, [None, CLASS_SIZE], name = 'y-input')\n",
    "    is_training = tf.placeholder(tf.bool, [])\n",
    "\n",
    "    \n",
    "    #model\n",
    "    filter_sizes = [2, 3, 4]\n",
    "    with tf.variable_scope('branch-x'):\n",
    "        with slim.arg_scope([slim.conv2d],\n",
    "                           activation_fn=tf.nn.elu,\n",
    "                           normalizer_fn=slim.batch_norm,\n",
    "                           normalizer_params={'is_training': is_training, 'decay':0.95}):\n",
    "            conv1 = slim.conv2d(x, 5, [1, 1], scope='conv1', padding='SAME')\n",
    "            print(conv1)\n",
    "            conv2_outputs = []\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                # Convolution Layer\n",
    "                name_scope = \"conv2-%s\" % filter_size\n",
    "                kernel_size = [filter_size, 2]\n",
    "                conv2 = slim.conv2d(conv1, 128, kernel_size, scope = name_scope, padding='SAME')\n",
    "                print(conv2)\n",
    "                conv2_outputs.append(conv2)\n",
    "            conv2 = slim.conv2d(conv1, 128, [1, 1], scope = \"conv2-1\", padding='SAME')\n",
    "            print(conv2)\n",
    "            conv2_outputs.append(conv2)\n",
    "            conv2_concat = tf.concat(conv2_outputs, 3)\n",
    "            print(conv2_concat)\n",
    "\n",
    "            conv3_outputs = []\n",
    "            for i, filter_size in enumerate(filter_sizes):\n",
    "                # Convolution Layer\n",
    "                name_scope = \"conv3-%s\" % filter_size\n",
    "                kernel_size = [filter_size, 2]\n",
    "                conv3 = slim.conv2d(conv2_concat, 256, kernel_size, scope = name_scope, padding='VALID')\n",
    "                pool = slim.max_pool2d(conv3, [FEATURE_SIZE - filter_size - 1, 0], scope = name_scope, padding='VALID')\n",
    "                print(pool)\n",
    "                conv3_outputs.append(pool)\n",
    "            conv3 = slim.conv2d(conv2_concat, 256, [1, 1], scope = \"conv3-1\", padding='SAME')\n",
    "            pool = slim.max_pool2d(conv3, [FEATURE_SIZE - 2, 0], scope = name_scope, padding='VALID')\n",
    "\n",
    "            print(pool)\n",
    "            conv3_outputs.append(pool)\n",
    "            conv3_concat = tf.concat(conv3_outputs, 3)\n",
    "            print(conv3_concat)\n",
    "\n",
    "    with slim.arg_scope([slim.conv2d],\n",
    "                           activation_fn=tf.nn.elu,\n",
    "                           normalizer_fn=slim.batch_norm,\n",
    "                           normalizer_params={'is_training': is_training, 'decay':0.95}):\n",
    "        conv4 = slim.conv2d(conv3_concat, CLASS_SIZE, [1, 1], scope = \"conv4\", padding = 'SAME')\n",
    "        pool = slim.max_pool2d(conv4, [2, 2], scope = \"conv4\", padding = 'VALID')\n",
    "        logits = tf.reduce_mean(pool, axis = [1, 2])\n",
    "        print(pool)\n",
    "        print(logits)\n",
    "        \n",
    "    correct_prediction = tf.equal(tf.argmax(logits, 1), tf.argmax(y_gt, 1))\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct_prediction, tf.float32))\n",
    "\n",
    "    cross_entropy = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=y_gt, logits=logits))\n",
    "    loss = cross_entropy\n",
    "#     , dtype=np.int32\n",
    "    step = tf.get_variable(\"step\", [], initializer=tf.constant_initializer(0.0), trainable=False)\n",
    "    learning_rate = tf.train.exponential_decay(LEARNING_RATE_BASE, step, BATCH_NUM, \n",
    "                                               LEARNING_RATE_DECAY, staircase = True)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate = learning_rate)\n",
    "    train_op = slim.learning.create_train_op(loss, optimizer, global_step=step)\n",
    "    update_ops = tf.get_collection(tf.GraphKeys.UPDATE_OPS)\n",
    "    if update_ops:\n",
    "        updates = tf.group(*update_ops)\n",
    "        loss = control_flow_ops.with_dependencies([updates], loss)\n",
    "\n",
    "    #Summary\n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    tf.summary.scalar('loss', loss)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    print(merged_summary_op)\n",
    "\n",
    "    \n",
    "    return{'x': x,\n",
    "       'y_gt': y_gt,\n",
    "       'is_training': is_training,\n",
    "       'train_op': train_op,\n",
    "       'global_step': step,\n",
    "       'accuracy': accuracy,\n",
    "       'loss': loss,\n",
    "       'summary': merged_summary_op}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train():\n",
    "    #loading data\n",
    "    ent_embedding = np.loadtxt('./data/ent_embeddings_transE.txt', delimiter=',')\n",
    "    rel_embedding = np.loadtxt('./data/rel_embeddings_transE.txt', delimiter=',')\n",
    "    train_id = np.loadtxt('./data/triple2id.txt', dtype = np.int32)\n",
    "    valid_id = np.loadtxt('./data/valid2id.txt', dtype = np.int32)\n",
    "\n",
    "    train_data, train_label = generate_input_list(ent_embedding, rel_embedding, train_id)\n",
    "    valid_data, valid_label = generate_input_list(ent_embedding, rel_embedding, valid_id)\n",
    "    \n",
    "    train_data, train_label = random_shuffle(train_data, train_label)\n",
    "    valid_data, valid_label = random_shuffle(valid_data, valid_label)\n",
    "    \n",
    "    global TRAIN_SIZE, FEATURE_SIZE, CLASS_SIZE, BATCH_SIZE, BATCH_NUM, EPOCH_SIZE\n",
    "    TRAIN_SIZE = train_data.shape[0]\n",
    "    FEATURE_SIZE = train_data.shape[1]\n",
    "    CLASS_SIZE = ent_embedding.shape[0]\n",
    "    BATCH_SIZE = 1\n",
    "    BATCH_NUM = TRAIN_SIZE/BATCH_SIZE + 1\n",
    "    EPOCH_SIZE = 50\n",
    "    \n",
    "    #load model\n",
    "    net = model()\n",
    "       \n",
    "    #start training\n",
    "    with tf.Session() as sess:\n",
    "        #write log\n",
    "        summary_writer = tf.summary.FileWriter('./log', sess.graph)\n",
    "        \n",
    "        #initial the variables\n",
    "        tf.global_variables_initializer().run()\n",
    "        \n",
    "        #Train cycle\n",
    "        for epoch in range(EPOCH_SIZE):\n",
    "            offset = 0\n",
    "            for i in range(BATCH_NUM):\n",
    "                offset = i * BATCH_SIZE\n",
    "                data_batch = train_data[offset:min((offset + BATCH_SIZE), TRAIN_SIZE)]\n",
    "                label_batch = preprocess_labels(train_label[offset:min((offset + BATCH_SIZE), TRAIN_SIZE)], ent_embedding.shape[0])\n",
    "                train_dict = {net['x']: data_batch,\n",
    "                             net['y_gt']: label_batch,\n",
    "                             net['is_training']: True}\n",
    "                step, _ = sess.run([net['global_step'], net['train_op']], feed_dict = train_dict)\n",
    "                print('sucess!')\n",
    "                break\n",
    "                if step % 50 == 0:\n",
    "                    loss, acc, summary = sess.run([net['loss'], net['accuracy'],net['summary']], feed_dict = train_dict)\n",
    "                    print('Train step{}; loss{}; accuracy{}'.format(step, loss, acc))\n",
    "                    summary_writer.add_summary(summary, step)\n",
    "                \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"branch-x/conv1/Elu:0\", shape=(256, 100, 3, 5), dtype=float32)\n",
      "Tensor(\"branch-x/conv2-2/Elu:0\", shape=(256, 100, 3, 128), dtype=float32)\n",
      "Tensor(\"branch-x/conv2-3/Elu:0\", shape=(256, 100, 3, 128), dtype=float32)\n",
      "Tensor(\"branch-x/conv2-4/Elu:0\", shape=(256, 100, 3, 128), dtype=float32)\n",
      "Tensor(\"branch-x/conv2-1/Elu:0\", shape=(256, 100, 3, 128), dtype=float32)\n",
      "Tensor(\"branch-x/concat:0\", shape=(256, 100, 3, 512), dtype=float32)\n",
      "Tensor(\"branch-x/conv3-2_1/MaxPool:0\", shape=(256, 2, 2, 256), dtype=float32)\n",
      "Tensor(\"branch-x/conv3-3_1/MaxPool:0\", shape=(256, 2, 2, 256), dtype=float32)\n",
      "Tensor(\"branch-x/conv3-4_1/MaxPool:0\", shape=(256, 2, 2, 256), dtype=float32)\n",
      "Tensor(\"branch-x/conv3-4_2/MaxPool:0\", shape=(256, 2, 2, 256), dtype=float32)\n",
      "Tensor(\"branch-x/concat_1:0\", shape=(256, 2, 2, 1024), dtype=float32)\n",
      "Tensor(\"conv4_1/MaxPool:0\", shape=(256, 1, 1, 14951), dtype=float32)\n",
      "Tensor(\"Mean:0\", shape=(256, 14951), dtype=float32)\n",
      "Tensor(\"Merge/MergeSummary:0\", shape=(), dtype=string)\n"
     ]
    }
   ],
   "source": [
    "train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-c7ff8a7d5979>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_label\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_input_list\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ment_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrel_embedding\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mdata_batch\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_data\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m256\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'data' is not defined"
     ]
    }
   ],
   "source": [
    "ent_embedding = np.loadtxt('./data/ent_embeddings_transE.txt', delimiter=',')\n",
    "rel_embedding = np.loadtxt('./data/rel_embeddings_transE.txt', delimiter=',')\n",
    "train_id = np.loadtxt('./data/triple2id.txt', dtype = np.int32)\n",
    "train_data, train_label = generate_input_list(ent_embedding, rel_embedding, train_id)\n",
    "data_batch = train_data[:256]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(256, 100, 3, 1)\n"
     ]
    }
   ],
   "source": [
    "print(data_batch.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
